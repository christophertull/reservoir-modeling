{
 "metadata": {
  "name": "",
  "signature": "sha256:024b00bb044e823c5b70b145c7e4992cc6b346c58cb18307ee35526587fa0dfd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "HTML Table Scraper"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the scraper to pull water sensor data from the [California Data Exchange Center](http://cdec.water.ca.gov/). \n",
      "\n",
      "The script first gathers lists of all monitoring locations that have instances of all desired sensor types.\n",
      "\n",
      "Times series data is then pulled from the sensors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2, re, os\n",
      "from pandas import DataFrame, Series\n",
      "import pandas as pd\n",
      "from bs4 import BeautifulSoup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sensor numbers were retrieved manually by doing a [station search](http://cdec.water.ca.gov/cgi-progs/staSearch) for the desired sensor type."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_nums = [2,15,23,76]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define the table scraping function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrapeTable(url,params,outname='output',cols=None, write=False):\n",
      "    \"\"\"Takes a url modified with params, opens the url, scrapes the table \n",
      "    there and then writes the table to a csv\n",
      "    \n",
      "    url:       a url with open spots for parameters\n",
      "    \n",
      "    params:    the parameters to be inserted.\n",
      "    \n",
      "    outname:   the name of the output file.\n",
      "    \n",
      "    cols:      optionally specify a subset of the columns to save.\n",
      "    \"\"\"\n",
      "    real_url = url%params\n",
      "    page = urllib2.urlopen(real_url).read()\n",
      "    soup = BeautifulSoup(page)\n",
      "    table = soup.find_all('table')[0]\n",
      "    \n",
      "    trs = table.find_all('tr')\n",
      "    firstRow = trs[0]\n",
      "    header = []\n",
      "    data = {}\n",
      "\n",
      "    #get the headers\n",
      "    for td in firstRow:\n",
      "        h = td.get_text().strip()\n",
      "        matches = re.split(r'[^a-zA-Z0-9]', h)\n",
      "        h = ' '.join( [s.strip() for s in matches if len(s)>=1] )\n",
      "        header.append( h )\n",
      "        data[h] = []\n",
      "    \n",
      "    #get the data\n",
      "    for tr in trs[1:]:\n",
      "        for i,td in enumerate(tr):\n",
      "            try:\n",
      "                text = td.get_text().strip()\n",
      "            except AttributeError:\n",
      "                text = None\n",
      "\n",
      "            data[header[i]].append(text)\n",
      "            \n",
      "    \n",
      "    df = DataFrame(data)\n",
      "    if cols != None:\n",
      "        df = df[cols]\n",
      "        \n",
      "    if write:\n",
      "        #get the suffix for the current table\n",
      "        text = soup.find_all('em')[0].get_text()\n",
      "        ls = re.split(r',*\\W+', text)\n",
      "        suffix = '_' + '_'.join(ls) + '.csv'    \n",
      "        \n",
      "        df.to_csv(outname+suffix, index=False)\n",
      "    \n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scrape lists of monitoring stations\n",
      "We need to scrape one table for each of the desired sensors (there is no option to AND multiple sensors)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "station_search_url = 'http://cdec.water.ca.gov/cgi-progs/staSearch?staid=&sensor_chk=on&sensor=%d&dur_chk=on&dur=D&active_chk=on&active=Y&display=staid'\n",
      "stationCols = ['ID','Latitude','Longitude','Station Name']\n",
      "outname = 'data\\stations_with'\n",
      "dflist = []\n",
      "\n",
      "for sens in sensor_nums:\n",
      "    df = scrapeTable(station_search_url,sens,outname,stationCols, write=True) \n",
      "    dflist.append(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now read in each of the lists and intersect them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regex = r'%s.+\\.csv' % 'stations_with'\n",
      "stationfiles = [f for f in os.listdir('data') if re.match(regex,f) ]\n",
      "\n",
      "# #read the station lists\n",
      "# for s in stationfiles:\n",
      "#     dflist.append(pd.read_csv('data\\\\'+s))\n",
      "    \n",
      "#intersect them\n",
      "df = dflist[0]\n",
      "for frame in dflist[1:]:\n",
      "    bools = frame.ID.isin(df.ID)\n",
      "    df = frame[bools]\n",
      "    \n",
      "#save result    \n",
      "df.to_csv(outname + '_all_sensors.csv')\n",
      "print df.head()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     ID  Latitude   Longitude            Station Name\n",
        "16  HTH  37.95000  -119.78300            HETCH HETCHY\n",
        "26  ORO  39.54000  -121.49300            OROVILLE DAM\n",
        "31  SHA  40.71800  -122.42000      SHASTA DAM  (USBR)\n",
        "35  WHI  40.59800  -122.53700  WHISKEYTOWN DAM (USBR)\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pull the time series data\n",
      "We now have a list of all stations that fit our desired sensor makeup. So lets pull the actual time series data from them!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_url = 'http://cdec.water.ca.gov/cgi-progs/selectQuery?station_id=%s&sensor_num=%d&dur_code=D&start_date=%s&end_date=%s'\n",
      "station_ids = df.ID.values\n",
      "startDate = '2004-11-01'\n",
      "endDate = '2014-11-01'\n",
      "\n",
      "for ID in station_ids[:1]:\n",
      "    \n",
      "    for sens in sensor_nums[:1]:\n",
      "        params = (ID,sens,startDate,endDate)\n",
      "        outfile = 'data/ts_'+ID\n",
      "        df = scrapeTable(data_url,params,outfile)\n",
      "        print df.head()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      Date Time RAININCHES\n",
        "0    10/01/2012       0.00\n",
        "1    10/02/2012       0.00\n",
        "2    10/03/2012       0.00\n",
        "3    10/04/2012       0.00\n",
        "4    10/05/2012       0.00\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}